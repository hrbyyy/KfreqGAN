# coding:--utf-8--

import sys
import torch
import torch.nn as nn
from torch.nn.utils import weight_norm
import os
import argparse

curPath = os.path.abspath(os.path.dirname(__file__))

fatherpath = os.path.split(curPath)[0]
grandpath=os.path.split(fatherpath)[0]
greatgrandpath=os.path.split(grandpath)[0]
gggpath=os.path.split(greatgrandpath)[0]
extra_path='/home/ices/PycharmProject/shape_sequence_kpi/uts/conditional_conv/model_combine/'
sys.path.append(fatherpath)
sys.path.append(curPath)
sys.path.append(grandpath)
sys.path.append(greatgrandpath)
sys.path.append(gggpath)
sys.path.append(os.path.split(gggpath)[0])
sys.path.append(extra_path)
import numpy as np
import pandas as pd
import torch.optim as optim
import torch.utils.data as Data
from torch.optim.lr_scheduler import MultiStepLR
import pickle
# import matplotlib.pyplot as plt
from functools import reduce
from torch.nn import BatchNorm1d
import time
import score_GAN
import gc

import math



datapath=''


def form_dataloader(batchsize,inpath,lb,lp):
    trainx_new=pickle.load(open(inpath+'trainx.pkl','rb'))
    val1x_new=pickle.load(open(inpath+'val1x.pkl','rb'))
    val2x_new = pickle.load(open(inpath +'val2x.pkl', 'rb'))
    testx_new = pickle.load(open(inpath + 'testx.pkl', 'rb'))

    trainy=pickle.load(open(inpath+'trainy.pkl','rb'))
    val1y = pickle.load(open(inpath + 'val1y.pkl', 'rb'))
    val2y = pickle.load(open(inpath +'val2y.pkl', 'rb'))
    testy = pickle.load(open(inpath + 'testy.pkl', 'rb'))

    train_kf = pickle.load(open(inpath +'train_kf.pkl', 'rb'))
    val1_kf = pickle.load(open(inpath +'val1_kf.pkl', 'rb'))
    val2_kf = pickle.load(open(inpath + 'val2_kf.pkl', 'rb'))
    test_kf = pickle.load(open(inpath +'test_kf.pkl', 'rb'))


    train_x = torch.from_numpy(trainx_new).type(torch.FloatTensor)
    val1_x = torch.from_numpy(val1x_new).type(torch.FloatTensor)
    val2_x = torch.from_numpy(val2x_new).type(torch.FloatTensor)
    test_x = torch.from_numpy(testx_new).type(torch.FloatTensor)

    train_y = torch.from_numpy(trainy).type(torch.FloatTensor)
    val1_y = torch.from_numpy(val1y).type(torch.FloatTensor)
    val2_y = torch.from_numpy(val2y).type(torch.FloatTensor)
    test_y = torch.from_numpy(testy).type(torch.FloatTensor)


    train_kf=torch.from_numpy(train_kf).unsqueeze(dim=2).repeat(1,1,lb).type(torch.FloatTensor) #(b,k)化为（b,t,k）
    val1_kf = torch.from_numpy(val1_kf).unsqueeze(dim=2).repeat(1,1,lb).type(torch.FloatTensor)  # (b,k)化为（b,t,k）
    val2_kf = torch.from_numpy(val2_kf).unsqueeze(dim=2).repeat(1,1,lb).type(torch.FloatTensor)  # (b,k)化为（b,t,k）
    test_kf = torch.from_numpy(test_kf).unsqueeze(dim=2).repeat(1,1,lb).type(torch.FloatTensor)  # (b,k)化为（b,t,k）

    train_x=torch.cat([train_x,train_kf],dim=1)
    val1_x = torch.cat([val1_x, val1_kf], dim=1)
    val2_x = torch.cat([val2_x, val2_kf], dim=1)
    test_x = torch.cat([test_x, test_kf], dim=1)

    torch_trainset = Data.TensorDataset(train_x, train_y)
  
    trainloader = Data.DataLoader(
        dataset=torch_trainset, 
        batch_size=batchsize, 
        shuffle=True  
    )
    val1data = Data.TensorDataset(val1_x, val1_y)
    val1loader = Data.DataLoader(dataset=val1data, shuffle=True, batch_size=batchsize)
    val2data = Data.TensorDataset(val2_x, val2_y)
    val2loader = Data.DataLoader(dataset=val2data, shuffle=False, batch_size=batchsize)
    testdata = Data.TensorDataset(test_x, test_y)
    testloader = Data.DataLoader(dataset=testdata, shuffle=False, batch_size=batchsize)

    return trainloader,val1loader,val2loader,testloader

class Options:
    def __init__(self,ngf,ndf,c_in,c_out,lb,lp):
        ##
        #
        self.ngf=ngf
        self.ndf=ndf
        # self.nz=nz
        # self.dnz=dnz
        self.c_in=c_in
        self.c_out=c_out
        self.lb=lb
        self.lp=lp



class Encoder(nn.Module):
    def __init__(self,opt):
        super(Encoder, self).__init__()
        layer1=nn.Conv1d(opt.c_in,opt.ngf,4,2,1,bias=False)
        BN1=nn.BatchNorm1d(opt.ngf)
        activation1=nn.ReLU(inplace=True)
        layer2=nn.Conv1d(opt.ngf,2*opt.ngf,4,2,1,bias=False)
        BN2=nn.BatchNorm1d(2*opt.ngf)
        activation2=nn.ReLU(inplace=True)
        self.encoder=nn.Sequential(layer1,BN1,activation1,layer2,BN2,activation2)


    def forward(self, x):
        z = self.encoder(x)
        return z



class Decoder(nn.Module):
    def __init__(self, opt):
        super(Decoder, self).__init__()
        layer1=nn.ConvTranspose1d(2*opt.ngf,opt.ngf,4,2,1,bias=False)
        BN1=nn.BatchNorm1d(opt.ngf)
        activation1=nn.ReLU(inplace=True)
        layer2=nn.Conv1d(opt.ngf,opt.c_out,10,1,0,bias=False)
        activation2=nn.Tanh()
        self.decoder=nn.Sequential(layer1,BN1,activation1,layer2,activation2)

    def forward(self, z):
        x=self.decoder(z)
        return x

class Generator(nn.Module):

    def __init__(self, opt):
        super(Generator, self).__init__()
        self.encoder1 = Encoder(opt)
        self.decoder = Decoder(opt)

    def forward(self, x):
        latent_i = self.encoder1(x)
        gen_x = self.decoder(latent_i)
        return gen_x, latent_i

class Discriminator(nn.Module):
    def __init__(self, opt):
        super(Discriminator, self).__init__()

        layer1=nn.ConvTranspose1d(opt.c_out,opt.ndf,4,2,1,bias=False)
        activation1=nn.LeakyReLU(0.2,inplace=True)
        layer2=nn.Conv1d(opt.ndf,opt.c_out,4,2,1,bias=False)
        BN2=nn.BatchNorm1d(opt.c_out)
        self.features=nn.Sequential(layer1,activation1)
        self.classifier=nn.Sequential(layer2,BN2)
    def forward(self, x):

        features = self.features(x)
        classifier = self.classifier(features)
        classifier = classifier.squeeze(dim=1)

        return classifier, features




def train(Generator,k, lambda_m,lambda_b,Discriminator,d_clip, device, trainloader,epoch, optimizer_g, optimizer_d):
    rloss_d=0
    running_loss_d=0
    rloss_g = 0
    running_loss_g = 0.0  #
    Generator.train()
    Discriminator.train()
    criterion_g=torch.nn.MSELoss(reduction='sum')
    criterion_d=torch.nn.BCELoss(reduction='sum')

    for i, data in enumerate(trainloader):

        inputs = data[0][:, :-k - 1, :]
        targets = data[1][:, :-1, :]
        kf = data[0][:, -k:, :]
        inputs = torch.cat([inputs, kf], dim=1)
        inputs = inputs.to(device)
        targets = targets.to(device)


        real_label = torch.ones(size=(targets.shape[0], targets.shape[2]), device=device)
        predict_label = torch.zeros(size=(targets.shape[0], targets.shape[2]), device=device)

        optimizer_g.zero_grad()
        outputs_g,_ = Generator(inputs)
        g_label,g_middle=Discriminator(outputs_g)
        real_out, real_middle = Discriminator(targets)

        loss_g_b = criterion_g(g_middle,real_middle)
        loss_g_m=criterion_g(outputs_g,targets)
        loss_g=lambda_m*loss_g_m+lambda_b*loss_g_b
        loss_g.backward()
        optimizer_g.step()
        rloss_g += loss_g.item()
        running_loss_g += loss_g.item()
        optimizer_d.zero_grad()
        real_out, real_middle = Discriminator(targets)
        d_loss_real =criterion_d(real_out, real_label)
        predict_out, predict_middle = Discriminator(outputs_g.detach())
        d_loss_fake = criterion_d(predict_out, predict_label)


        d_loss = d_loss_real + d_loss_fake
        d_loss.backward()

        optimizer_d.step()
        for para in Discriminator.parameters():
            if para.requires_grad_ == True:
                torch.nn.utils.clip_grad_value_(para, d_clip)

        rloss_d += d_loss
        running_loss_d += d_loss

        if i % 20 == 19:  # print every  20 steps
            print('[%d, %5d] G train loss: %.3f   D train loss:%.3f'  %
                  (epoch + 1, i + 1, running_loss_g / 20,running_loss_d/20))
            running_loss_g = 0.0
            running_loss_d=0.0
    print('[%d] G train loss: %.3f   D train loss:%.3f' %
          (epoch + 1,  rloss_g , rloss_d / 20))

    torch.cuda.empty_cache()
    del trainloader
    gc.collect()
    return







if __name__ == "__main__":

    repeats = 5
    epoches = 100
    d_clip = 0.01
    pw=3
    k =   3
    hw = 24

    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    batchsize = 64

    input_dim = 1
    k_freq = 3


    output_dim = input_dim

    lambda_b = 0.01
    lambda_m = 1
    patience = 20
    threshold = 2
    p0 = ''
    if os.path.exists(p0) == False:
        os.mkdir(p0, 0o777)
    p0=p0+'k'+str(k)+'/'
    if os.path.exists(p0) == False:
        os.mkdir(p0, 0o777)
    p0=p0+'hw'+str(hw)+'/'
    if os.path.exists(p0) == False:
        os.mkdir(p0, 0o777)

    datapath=datapath+'hw'+str(hw)+'pw'+str(pw)+'/'
    opt = Options(ngf=8, ndf=8,  c_in=4, c_out=1, lb=hw, lp=pw)


    for r in range(repeats):
        outp = p0 + str(r) + '/'
        if os.path.exists(outp) == False:
            os.mkdir(outp, 0o777)


        generator = Generator(opt)
        generator = nn.DataParallel(generator)
        generator.to(device)
        discriminator = Discriminator(opt)
        discriminator = nn.DataParallel(discriminator)
        discriminator.to(device)
        optimizer_g = optim.RMSprop(generator.parameters(), lr=1e-4)
        optimizer_d = optim.RMSprop(discriminator.parameters(), lr=1e-4)
    
        for epoch in range(epoches):
            trainloader, val1loader, val2loader, testloader = form_dataloader(batchsize, datapath, hw,pw)
            train(generator, k_freq, lambda_m, lambda_b, discriminator, d_clip, device,
                trainloader,  epoch,  optimizer_g,optimizer_d)

         
         







